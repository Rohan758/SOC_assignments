{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9ced023-04e8-4ac0-b7af-713b93c377bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e109e578-9498-4ccb-9305-4f3d403418f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation(object):\n",
    "\n",
    "\n",
    "    def __init__(self):\n",
    "        self.state = None\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        raise NotImplemented\n",
    "\n",
    "    def derivative(self):\n",
    "        raise NotImplemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3280097b-f5c6-4d52-be95-351f280f318c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Identity(Activation):\n",
    "\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Identity, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.state = x\n",
    "        return x\n",
    "\n",
    "    def derivative(self):\n",
    "        return 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb2855b6-5f50-4a4a-95f7-9047a5c6c8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(Activation):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Sigmoid, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.state = 1 / (1 + np.exp(-x))\n",
    "        return self.state\n",
    "    \n",
    "    def derivative(self):\n",
    "        return self.state * (1 - self.state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8a06ae5-b374-47db-8fc6-7063b3a10d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh(Activation):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Tanh, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.state = np.tanh(x)\n",
    "        return self.state\n",
    "    \n",
    "    def derivative(self):\n",
    "        return 1 - self.state**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a209b87d-33c1-4675-bbb4-56c740ceedce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Activation):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(ReLU, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.state = np.maximum(0, x)\n",
    "        return self.state\n",
    "    \n",
    "    def derivative(self):\n",
    "        return np.where(self.state > 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b603faf1-5c03-4615-9cbb-12835049bb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Criterion(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.logits = None\n",
    "        self.labels = None\n",
    "        self.loss = None\n",
    "\n",
    "    def __call__(self, x, y):\n",
    "        return self.forward(x, y)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        raise NotImplemented\n",
    "\n",
    "    def derivative(self):\n",
    "        raise NotImplemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fed64078-f0f4-4c2e-924d-ffaee84cae10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxCrossEntropy():\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(SoftmaxCrossEntropy, self).__init__()\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        self.logits = x\n",
    "        self.labels = y\n",
    "        mx = np.max(self.logits, axis-1).reshape(-1,1)\n",
    "        subtracted = self.logits - mx\n",
    "        self.elogits = np.exp(subtracted)\n",
    "        esum = self.elogits.sum(axis=1).reshape(-1,1)\n",
    "        self.sm = self.elogits / esum\n",
    "        \n",
    "        fterm = -(self.logits*self.labels).sum(axis=1)\n",
    "        sterm = mx +np.log(esum)\n",
    "        return fterm + sterm.reshape(-1)\n",
    "\n",
    "    \n",
    "    def derivative(self):\n",
    "\n",
    "        return self.sm - self.labels\n",
    "\n",
    "       \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30b61f7c-5415-4923-94a3-0fba0c675b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear():\n",
    "    def __init__(self, in_feature, out_feature, weight_init_fn, bias_init_fn):\n",
    "\n",
    "        \"\"\"\n",
    "        Argument:\n",
    "            W (np.array): (in feature, out feature)\n",
    "            dW (np.array): (in feature, out feature)\n",
    "            momentum_W (np.array): (in feature, out feature)\n",
    "\n",
    "            b (np.array): (1, out feature)\n",
    "            db (np.array): (1, out feature)\n",
    "            momentum_B (np.array): (1, out feature)\n",
    "        \"\"\"\n",
    "\n",
    "        self.W = weight_init_fn(in_feature, out_feature)\n",
    "        self.b = bias_init_fn(out_feature)\n",
    "\n",
    "        # TODO: Complete these but do not change the names.\n",
    "        self.dW = np.zeros(self.w.shape)\n",
    "        self.db = np.zeros(self.b.shape)\n",
    "\n",
    "        self.momentum_W = np.zeros(self.w.shape)\n",
    "        self.momentum_b = np.zeros(self.b.shape)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Argument:\n",
    "            x (np.array): (batch size, in feature)\n",
    "        Return:\n",
    "            out (np.array): (batch size, out feature)\n",
    "        \"\"\"\n",
    "        self.x = x\n",
    "        out = np.matmul(self.x, self.w) + self.b\n",
    "\n",
    "        return out\n",
    "        \n",
    "\n",
    "    def backward(self, delta):\n",
    "\n",
    "        \"\"\"\n",
    "        Argument:\n",
    "            delta (np.array): (batch size, out feature)\n",
    "        Return:\n",
    "            out (np.array): (batch size, in feature)\n",
    "        \"\"\"\n",
    "        self.dw = np.dpt(self.x.T, delta) / delta.shape[0]\n",
    "        self.db = np.sum(delta, axis=0, keepdims=True) / delta.shape[0]\n",
    "        dx = np.dot(delta, self.w.T)\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ca59518-ddbb-4d9d-9e8e-df045e2b1ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(object):\n",
    "\n",
    "    \"\"\"\n",
    "    A simple multilayer perceptron\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, output_size, hiddens, activations, weight_init_fn,\n",
    "                 bias_init_fn, criterion, lr):\n",
    "\n",
    "        # Don't change this -->\n",
    "        self.train_mode = True\n",
    "        self.nlayers = len(hiddens) + 1\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.activations = activations\n",
    "        self.criterion = criterion\n",
    "        self.lr = lr\n",
    "        # <---------------------\n",
    "\n",
    "        # Don't change the name of the following class attributes,\n",
    "        # the autograder will check against these attributes. But you will need to change\n",
    "        # the values in order to initialize them correctly\n",
    "\n",
    "        # Initialize and add all your linear layers into the list 'self.linear_layers'\n",
    "        # (HINT: self.foo = [ bar(???) for ?? in ? ])\n",
    "        # (HINT: Can you use zip here?)\n",
    "        self.linear_layers = [Linear(inf,outf, weight_init_fn, bias_init_fn) for inf, outf in zip([self.input_size] + hiddens, hiddens + [self.output_size])]\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, layer in enumerate(self.liner_layers):\n",
    "            x = layer(x)\n",
    "            x = self.activations[i](x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    \n",
    "\n",
    "    def zero_grads(self):\n",
    "        # Use numpyArray.fill(0.0) to zero out your backpropped derivatives in each\n",
    "        # of your linear and batchnorm layers.\n",
    "        for layer in self.linear_layers:\n",
    "            layer.dw.fill(0,0)\n",
    "            layer.db.fill(0,0)\n",
    "            \n",
    "\n",
    "    def step(self):\n",
    "        # Apply a step to the weights and biases of the linear layers.\n",
    "        # (You will add momentum later in the assignment to the linear layers)\n",
    "\n",
    "        for i in range(len(self.linear_layers)):\n",
    "            layer = self.linear_layers[i]\n",
    "            layer.w = layer.w -self.lr * layer.dw\n",
    "            layer.b = layer.b -self.lr * layer.db\n",
    "\n",
    "    def backward(self, labels):\n",
    "        # Backpropagate through the activation functions, batch norm and\n",
    "        # linear layers.\n",
    "        # Be aware of which return derivatives and which are pure backward passes\n",
    "        # i.e. take in a loss w.r.t it's output.\n",
    "        final_layer = self.activations[-1]\n",
    "        final_outputs = final_layer.state\n",
    "        loss = self.criterion(final_outputs, labels)\n",
    "        delta = self.criterion.derivative()\n",
    "\n",
    "        for i in range(self.nlayers - 1, -1, -1):\n",
    "            delta = delta * self.activations[i].derivative()\n",
    "            delta = self.linear_layers[i].backward(delta)\n",
    "            \n",
    "\n",
    "    def error(self, labels):\n",
    "                return (np.argmax(self.output, axis = 1) != np.argmax(labels, axis = 1)).sum()\n",
    "\n",
    "    def total_loss(self, labels):\n",
    "        return self.criterion(self.output, labels).sum()\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "\n",
    "    def train(self):\n",
    "        self.train_mode = True\n",
    "\n",
    "    def eval(self):\n",
    "        self.train_mode = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15933acf-c533-4fd0-8523-5ea40080722b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
